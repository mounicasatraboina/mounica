{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPex1J4G77IJCiVeyTjh6KD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mounicasatraboina/mounica/blob/main/Inclassexcercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkCu_xXpfMiE"
      },
      "source": [
        "The third In-class-exercise (9/15/2021, 40 points in total)\n",
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis.\n",
        "\n",
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data.\n",
        "\n",
        "[ ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGz4fX4SfQCi"
      },
      "source": [
        "I am more interested to know about the devices that is recently published in the market.One such device is Apple Airpods Max. \n",
        "I want to know the “Apple Airpods Max reviews and ratings”.\n",
        "\n",
        "Any device can be ordered online  on different online platforms like Amazon Flipkart etc. purchase the iphones from the Apple website. Thet rate and review the product based on customer name,star, review posted and date and time. To know the ratings and reviews they at least extract 2000 units to know them.\n",
        "Steps that are needed to collect and save the data.\n",
        "1. To extract the reviews from the Apple website i use beautiful soup library.\n",
        "2. The reviews will be extracted from the page source using classname and appended to the program's empty list.\n",
        "3. To extract pattern 2000 reviews where I want to iterate it for min of one hundred times and every web page would have the critiques of 20 and generated the url dynamically.\n",
        "4. Then I created a dataframe and converted dataframe into csv.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMQ1sXdLfY9Q"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vbOoHMmfapl",
        "outputId": "b954ec2b-909a-405f-94ec-a30f7e0ca4e1"
      },
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import pandas as pd\n",
        "import requests as rq\n",
        "\n",
        "def imdb_webscraper(link):\n",
        "    df = pd.DataFrame(columns=['User name', 'Star', 'Review title', 'Review text', 'Review posted time'])\n",
        "    req = rq.get(link).text\n",
        "    soup = bs(req,'html.parser')\n",
        "\n",
        "    user_names = soup.find_all('span', attrs={'class' : 'display-name-link'})\n",
        "    star = soup.find_all('span', attrs={'class' : 'rating-other-user-rating'})\n",
        "    review_title = soup.find_all('a', attrs={'class' : 'title'})\n",
        "    review_text = soup.find_all('div', attrs={'class' : 'text show-more__control'})\n",
        "    review_posted_time = soup.find_all('span', attrs={'class' : 'review-date'})\n",
        "\n",
        "    list = []\n",
        "    for i in range(len(user_names)):\n",
        "        list.append(user_names[i].get_text())\n",
        "    df['User name'] = list\n",
        "\n",
        "    \"\"\"list = []\n",
        "    for i in range(len(star)):\n",
        "        list.append(star[i].get_text()[6])\n",
        "    df['Star'] = list\"\"\"\n",
        "\n",
        "    list = []\n",
        "    for i in range(len(review_title)):\n",
        "        list.append(review_title[i].get_text())\n",
        "    df['Review title'] = list\n",
        "\n",
        "    list = []\n",
        "    for i in range(len(review_text)):\n",
        "        list.append(review_text[i].get_text())\n",
        "    df['Review text'] = list\n",
        "\n",
        "    list = []\n",
        "    for i in range(len(review_posted_time)):\n",
        "        list.append(review_posted_time[i].get_text())\n",
        "    df['Review posted time'] = list\n",
        "    print(df.head())\n",
        "    df.to_csv(\"output.csv\")\n",
        "\n",
        "imdb_webscraper('https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         User name  ... Review posted time\n",
            "0       mysticfall  ...    18 October 2019\n",
            "1  Jeremy_Urquhart  ...        5 July 2019\n",
            "2      jtindahouse  ...     6 October 2019\n",
            "3    nehpetstephen  ...     25 August 2019\n",
            "4        keezo9uno  ...     19 August 2019\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvEI3djZfqf-"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "q3HR-DgTeTyV",
        "outputId": "098967ba-3ce9-42a1-813f-f5c154ac6e27"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests, lxml, os, json\n",
        "import pandas as pd\n",
        "\n",
        "headers = {\n",
        "    'User-agent':\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
        "}\n",
        "\n",
        "params = {\n",
        "  \"q\": \"datascience\",\n",
        "  \"hl\": \"en\",\n",
        "}\n",
        "html_link = requests.get('https://scholar.google.com/scholar', headers=headers, params = params).text\n",
        "sp = BeautifulSoup(html_link, 'lxml')\n",
        "for pdflink in sp.select('.gs_or_ggsm a'):\n",
        "  file_link = pdflink['href']\n",
        "  print(file_link)\n",
        "  \n",
        "data = []\n",
        "for res in sp.select('.gs_ri'):\n",
        "  Heading_of_the_title_name = res.select_one('.gs_rt').text\n",
        "  data_published = res.select_one('.gs_a').text\n",
        "  Review_data = res.select_one('.gs_rs').text\n",
        "  try:\n",
        "    Journal_reviews = res.select_one('a~ a+ .gs_nph')['href']\n",
        "  except:\n",
        "    Journal_reviews = None\n",
        "  data_f = {\n",
        "      'Title': Heading_of_the_title_name,\n",
        "      'Abstract': Review_data,\n",
        "      'Authors': data_published\n",
        "  }\n",
        " \n",
        "  data.append(data_f)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://archive.nyu.edu/bitstream/2451/31553/2/Dhar-DataScience.pdf\n",
            "https://nnov.hse.ru/data/2017/03/15/1112283148/program-1732155265-xsPmoe7O2q.pdf\n",
            "https://dl.acm.org/doi/pdf/10.1145/3076253\n",
            "http://www.verigazeteciligi.com/wp-content/uploads/2014/12/What_is_Data_Science.pdf\n",
            "http://repository.psa.edu.my/xmlui/bitstream/handle/123456789/2016/2017_Book_IntroductionToDataScience.pdf?sequence=1&isAllowed=y\n",
            "https://www.pnas.org/content/pnas/114/33/8689.full.pdf\n",
            "/scholar?output=instlink&q=info:0156YemMdDoJ:scholar.google.com/&hl=en&as_sdt=0,5&scillfp=8537815161385692053&oi=lle\n",
            "https://www.tandfonline.com/doi/pdf/10.1080/10618600.2017.1384734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Authors</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data science and prediction</td>\n",
              "      <td>Data science is the study of the generalizable...</td>\n",
              "      <td>V Dhar - Communications of the ACM, 2013 - dl....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data science in action</td>\n",
              "      <td>In recent years, data science emerged as a new...</td>\n",
              "      <td>W Van Der Aalst - Process mining, 2016 - Springer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data science: a comprehensive overview</td>\n",
              "      <td>The 21st century has ushered in the age of big...</td>\n",
              "      <td>L Cao - ACM Computing Surveys (CSUR), 2017 - d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[BOOK][B] What is data science?</td>\n",
              "      <td>We've all heard it: according to Hal Varian, s...</td>\n",
              "      <td>M Loukides - 2011 - books.google.com</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[BOOK][B] Data Science for Business: What you ...</td>\n",
              "      <td>Written by renowned data science experts Foste...</td>\n",
              "      <td>F Provost, T Fawcett - 2013 - books.google.com</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  ...                                            Authors\n",
              "0                        Data science and prediction  ...  V Dhar - Communications of the ACM, 2013 - dl....\n",
              "1                             Data science in action  ...  W Van Der Aalst - Process mining, 2016 - Springer\n",
              "2             Data science: a comprehensive overview  ...  L Cao - ACM Computing Surveys (CSUR), 2017 - d...\n",
              "3                    [BOOK][B] What is data science?  ...               M Loukides - 2011 - books.google.com\n",
              "4  [BOOK][B] Data Science for Business: What you ...  ...     F Provost, T Fawcett - 2013 - books.google.com\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaKenjUmhSV8"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOK0w228hBZ3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}